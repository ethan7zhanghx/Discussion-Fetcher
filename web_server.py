#!/usr/bin/env python3
"""Web Server for DiscussionFetcher - Êèê‰æõ Web ÁïåÈù¢Âíå API"""

from flask import Flask, render_template, jsonify, request, send_file
from flask_cors import CORS
from pathlib import Path
import os
import sys
from datetime import datetime
import threading
import time

# Ê∑ªÂä† src Âà∞ path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.database import DatabaseManager
from src.reddit import RedditFetcher
from src.huggingface import HuggingFaceFetcher

app = Flask(__name__, template_folder='web/templates', static_folder='web/static')
CORS(app)

# ÂÖ®Â±ÄÂèòÈáè
db = DatabaseManager()
fetch_status = {
    'running': False,
    'platform': None,
    'progress': '',
    'error': None
}


@app.route('/')
def index():
    """‰∏ªÈ°µ"""
    return render_template('index.html')


@app.route('/api/stats')
def get_stats():
    """Ëé∑ÂèñÊï∞ÊçÆÂ∫ìÁªüËÆ°"""
    try:
        stats = db.get_stats_detailed()

        # Ëé∑ÂèñÊúÄËøëÊõ¥Êñ∞Êó∂Èó¥
        recent = db.get_recent_discussions(limit=1)
        last_update = recent[0]['fetched_at'] if recent else None

        return jsonify({
            'success': True,
            'data': {
                'total': stats.get('total', 0),
                'platforms': stats.get('platforms', {}),
                'content_types': stats.get('content_types', {}),
                'last_update': last_update
            }
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/discussions')
def get_discussions():
    """Ëé∑ÂèñËÆ®ËÆ∫ÂàóË°®"""
    try:
        platform = request.args.get('platform')
        content_type = request.args.get('content_type')
        search_keywords = request.args.get('search_keywords')
        limit = int(request.args.get('limit', 100))
        offset = int(request.args.get('offset', 0))

        discussions = db.query_discussions(
            platform=platform,
            content_type=content_type,
            search_keywords=search_keywords,
            limit=limit,
            offset=offset
        )

        return jsonify({
            'success': True,
            'data': discussions,
            'count': len(discussions)
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/search')
def search_discussions():
    """ÊêúÁ¥¢ËÆ®ËÆ∫"""
    try:
        keyword = request.args.get('keyword', '')
        platform = request.args.get('platform')
        limit = int(request.args.get('limit', 100))

        if not keyword:
            return jsonify({'success': False, 'error': 'Keyword required'}), 400

        discussions = db.search_discussions(
            keyword=keyword,
            platform=platform,
            limit=limit
        )

        return jsonify({
            'success': True,
            'data': discussions,
            'count': len(discussions)
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/export')
def export_data():
    """ÂØºÂá∫Êï∞ÊçÆ"""
    try:
        format_type = request.args.get('format', 'csv')
        platform = request.args.get('platform')
        search_keywords = request.args.get('search_keywords')

        # ÊûÑÂª∫Êñá‰ª∂Âêç
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        parts = ['discussions']
        if platform:
            parts.append(platform)
        if search_keywords:
            parts.append(search_keywords.replace(' ', '_'))
        parts.append(timestamp)
        filename = f'{"_".join(parts)}.{format_type}'
        filepath = Path('./data/exports') / filename
        filepath.parent.mkdir(parents=True, exist_ok=True)

        # ÂØºÂá∫
        if format_type == 'csv':
            db.export_to_csv(str(filepath), platform=platform, search_keywords=search_keywords, limit=None)
        elif format_type == 'excel':
            # Excel ÂØºÂá∫ÈúÄË¶ÅÁâπÊÆäÂ§ÑÁêÜ
            if platform:
                db.export_to_excel(str(filepath), platforms=[platform], search_keywords=search_keywords)
            else:
                db.export_to_excel(str(filepath), search_keywords=search_keywords)
        else:
            return jsonify({'success': False, 'error': 'Invalid format'}), 400

        return send_file(filepath, as_attachment=True)

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/fetch/start', methods=['POST'])
def start_fetch():
    """ÂºÄÂßãÊäìÂèñÊï∞ÊçÆÔºàÂêéÂè∞‰ªªÂä°Ôºâ"""
    global fetch_status

    if fetch_status['running']:
        return jsonify({
            'success': False,
            'error': 'Fetch already running'
        }), 400

    try:
        data = request.json
        platforms = data.get('platforms', ['reddit', 'huggingface'])
        query = data.get('query', 'ERNIE')
        include_comments = data.get('include_comments', False)
        reddit_search_mode = data.get('reddit_search_mode', 'subreddits')

        # ÂêØÂä®ÂêéÂè∞Á∫øÁ®ã
        thread = threading.Thread(
            target=fetch_worker,
            args=(platforms, query, include_comments, reddit_search_mode)
        )
        thread.daemon = True
        thread.start()

        return jsonify({
            'success': True,
            'message': 'Fetch started'
        })

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/fetch/status')
def fetch_status_api():
    """Ëé∑ÂèñÊäìÂèñÁä∂ÊÄÅ"""
    return jsonify({
        'success': True,
        'data': fetch_status
    })


def fetch_worker(platforms, query, include_comments, reddit_search_mode='subreddits'):
    """ÂêéÂè∞ÊäìÂèñ‰ªªÂä°"""
    global fetch_status

    fetch_status['running'] = True
    fetch_status['error'] = None

    try:
        # Reddit
        if 'reddit' in platforms:
            fetch_status['platform'] = 'reddit'

            reddit = RedditFetcher(verbose=False)

            # Ê†πÊçÆÊêúÁ¥¢ÊñπÂºèÈÄâÊã©‰∏çÂêåÁöÑÊäìÂèñÊñπÊ≥ï
            if reddit_search_mode == 'global':
                fetch_status['progress'] = f'üîç Reddit ÂÖ®Â±ÄÊêúÁ¥¢ÂÖ≥ÈîÆËØç: {query}'
                time.sleep(0.5)  # ËÆ©ÂâçÁ´ØÊúâÊó∂Èó¥ÊçïÊçâËøô‰∏™Áä∂ÊÄÅ

                # ÂÖ®Â±ÄÊêúÁ¥¢
                posts = reddit.search_all_reddit(
                    query=query,
                    limit=100,
                    search_keywords=query
                )

                fetch_status['progress'] = f'‚úì Reddit: ÊâæÂà∞ {len(posts)} Êù°Â∏ñÂ≠ê'
                time.sleep(0.5)

                # Ëé∑ÂèñËØÑËÆ∫
                if include_comments and posts:
                    total_comments = 0
                    for idx, post in enumerate(posts, 1):
                        fetch_status['progress'] = f'üì• Ëé∑ÂèñËØÑËÆ∫ ({idx}/{len(posts)}): {post.title[:30]}...'
                        comments = reddit.fetch_post_comments(
                            post_id=post.id,
                            post_title=post.title,
                            subreddit_name=post.subreddit,
                            search_keywords=query,
                            replace_more_limit=0  # ÂÖ®Â±ÄÊêúÁ¥¢Êó∂Â±ïÂºÄÊâÄÊúâËØÑËÆ∫
                        )
                        total_comments += len(comments)

                    fetch_status['progress'] = f'‚úì Reddit: Ëé∑Âèñ‰∫Ü {total_comments} Êù°ËØÑËÆ∫'
                    time.sleep(0.5)
            else:
                fetch_status['progress'] = f'üîç Reddit Â≠êÁâàÂùóÊêúÁ¥¢ÂÖ≥ÈîÆËØç: {query}'
                time.sleep(0.5)

                # ÁâπÂÆöÂ≠êÁâàÂùóÊêúÁ¥¢ÔºàÂéüÊúâÊñπÂºèÔºâ
                reddit.fetch(query=query, fetch_comments=include_comments)

                fetch_status['progress'] = f'‚úì Reddit: Â≠êÁâàÂùóÊêúÁ¥¢ÂÆåÊàê'
                time.sleep(0.5)

            # Selenium CommentsÔºà‰ªÖÂú®ÁâπÂÆöÂ≠êÁâàÂùóÊ®°Âºè + ‰∏çËá™Âä®Ëé∑ÂèñËØÑËÆ∫Êó∂ÂèØÁî®Ôºâ
            # Ê≥®ÊÑèÔºöÂÖ®Â±ÄÊêúÁ¥¢Ê®°Âºè‰∏ãÔºåÂ¶ÇÊûúÂãæÈÄâ‰∫Ü"Ëé∑ÂèñËØÑËÆ∫"ÔºåÂ∑≤ÁªèÂú®‰∏äÈù¢Áî® PRAW Ëé∑ÂèñËøá‰∫Ü
            # ËøôÈáåÁöÑ Selenium ÊñπÂºèÊòØÈíàÂØπÁâπÂÆöÂ≠êÁâàÂùóÁöÑÈ¢ùÂ§ñËØÑËÆ∫Ëé∑ÂèñÊñπÂºèÔºàÈúÄË¶Å cookiesÔºâ
            # Áî±‰∫éÈÄªËæëÂ§çÊùÇÔºåËøôÈáåÊöÇÊó∂Ë∑≥Ëøá Selenium ËØÑËÆ∫Ëé∑ÂèñÔºåÂÖ®ÈÉ®‰ΩøÁî® PRAW ÁöÑÊñπÂºè

        # HuggingFace
        if 'huggingface' in platforms:
            fetch_status['platform'] = 'huggingface'
            fetch_status['progress'] = f'üîç HuggingFace ÊêúÁ¥¢Ê®°Âûã: {query}'
            time.sleep(0.5)

            hf = HuggingFaceFetcher(verbose=False)
            discussions = hf.fetch(query)

            fetch_status['progress'] = f'‚úì HuggingFace: Ëé∑Âèñ‰∫Ü {len(discussions)} Êù°ËÆ®ËÆ∫'
            time.sleep(0.5)

        fetch_status['progress'] = 'üéâ ÊâÄÊúâÂπ≥Âè∞ÊäìÂèñÂÆåÊàêÔºÅ'
        time.sleep(0.5)

    except Exception as e:
        fetch_status['error'] = str(e)
        fetch_status['progress'] = f'Error: {e}'

    finally:
        fetch_status['running'] = False


@app.route('/api/cookies/check')
def check_cookies():
    """Ê£ÄÊü• cookies Êñá‰ª∂ÊòØÂê¶Â≠òÂú®"""
    cookies_path = Path('./cookies.json')
    return jsonify({
        'success': True,
        'exists': cookies_path.exists()
    })


@app.route('/api/keywords')
def get_keywords():
    """Ëé∑ÂèñÊâÄÊúâÊêúÁ¥¢ÂÖ≥ÈîÆËØçÂàóË°®"""
    try:
        keywords = db.get_search_keywords()
        return jsonify({
            'success': True,
            'data': keywords
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/twitter/import', methods=['POST'])
def import_twitter_csv():
    """ÂØºÂÖ• Twitter CSV Êñá‰ª∂"""
    try:
        if 'files' not in request.files:
            return jsonify({'success': False, 'error': 'No files uploaded'}), 400

        files = request.files.getlist('files')
        if not files:
            return jsonify({'success': False, 'error': 'No files uploaded'}), 400

        # Ëé∑ÂèñÊêúÁ¥¢ÂÖ≥ÈîÆËØçÔºàÂèØÈÄâÔºâ
        search_keywords = request.form.get('keywords', None)

        # ÂàõÂª∫‰∏¥Êó∂ÁõÆÂΩï
        temp_dir = Path('./data/temp_uploads')
        temp_dir.mkdir(parents=True, exist_ok=True)

        # ‰øùÂ≠òÊñá‰ª∂
        saved_files = []
        for file in files:
            if file.filename.endswith('.csv'):
                filepath = temp_dir / file.filename
                file.save(str(filepath))
                saved_files.append(str(filepath))

        if not saved_files:
            return jsonify({'success': False, 'error': 'No valid CSV files'}), 400

        # ÂØºÂÖ•Êï∞ÊçÆ
        from src.twitter_importer import TwitterCSVImporter
        importer = TwitterCSVImporter(verbose=False, search_keywords=search_keywords)
        total_count = importer.import_multiple_files(saved_files)

        # Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂
        for filepath in saved_files:
            try:
                os.remove(filepath)
            except:
                pass

        return jsonify({
            'success': True,
            'data': {
                'success_count': total_count,
                'files_processed': len(saved_files)
            }
        })

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/stats/timeline')
def get_timeline_stats():
    """Ëé∑ÂèñÊó∂Èó¥Á∫øÁªüËÆ°Êï∞ÊçÆÔºàÊåâÊó•/Âë®/ÊúàÂàÜÁªÑÔºâ"""
    try:
        group_by = request.args.get('group_by', 'day')  # day, week, month
        platform = request.args.get('platform')
        days = int(request.args.get('days', 30))  # ÈªòËÆ§ÊúÄËøë30Â§©

        # Ëé∑ÂèñÁªüËÆ°Êï∞ÊçÆ
        from datetime import datetime, timedelta
        import sqlite3

        cutoff_date = datetime.now() - timedelta(days=days)

        # ÊûÑÂª∫Êü•ËØ¢
        conn = sqlite3.connect(db.db_path)
        cursor = conn.cursor()

        # Ê†πÊçÆÂàÜÁªÑÊñπÂºèËÆæÁΩÆSQLÊó•ÊúüÊ†ºÂºè
        date_formats = {
            'day': '%Y-%m-%d',
            'week': '%Y-W%W',
            'month': '%Y-%m'
        }
        date_format = date_formats.get(group_by, '%Y-%m-%d')

        # ÊûÑÂª∫WHEREÂ≠êÂè•
        where_clauses = [f"created_at >= ?"]
        params = [cutoff_date.isoformat()]

        if platform:
            where_clauses.append("platform = ?")
            params.append(platform)

        where_sql = " AND ".join(where_clauses)

        # ÊâßË°åÊü•ËØ¢
        query = f"""
            SELECT
                strftime('{date_format}', created_at) as date_group,
                COUNT(*) as count,
                platform,
                content_type
            FROM discussions
            WHERE {where_sql}
            GROUP BY date_group, platform, content_type
            ORDER BY date_group
        """

        cursor.execute(query, params)
        results = cursor.fetchall()
        conn.close()

        # Ê†ºÂºèÂåñÁªìÊûú
        timeline_data = {}
        for row in results:
            date_group, count, plat, ctype = row
            if date_group not in timeline_data:
                timeline_data[date_group] = {'total': 0, 'by_platform': {}, 'by_type': {}}

            timeline_data[date_group]['total'] += count
            timeline_data[date_group]['by_platform'][plat] = timeline_data[date_group]['by_platform'].get(plat, 0) + count
            timeline_data[date_group]['by_type'][ctype] = timeline_data[date_group]['by_type'].get(ctype, 0) + count

        return jsonify({
            'success': True,
            'data': timeline_data,
            'group_by': group_by,
            'days': days
        })

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/stats/top_authors')
def get_top_authors():
    """Ëé∑ÂèñÊúÄÊ¥ªË∑ÉÁöÑ‰ΩúËÄÖÁªüËÆ°"""
    try:
        platform = request.args.get('platform')
        limit = int(request.args.get('limit', 20))
        days = int(request.args.get('days', 30))

        from datetime import datetime, timedelta
        import sqlite3

        cutoff_date = datetime.now() - timedelta(days=days)

        conn = sqlite3.connect(db.db_path)
        cursor = conn.cursor()

        # ÊûÑÂª∫Êü•ËØ¢
        where_clauses = [f"created_at >= ?", "author != '[deleted]'"]
        params = [cutoff_date.isoformat()]

        if platform:
            where_clauses.append("platform = ?")
            params.append(platform)

        where_sql = " AND ".join(where_clauses)
        params.append(limit)

        query = f"""
            SELECT
                author,
                COUNT(*) as post_count,
                platform,
                MAX(created_at) as last_post
            FROM discussions
            WHERE {where_sql}
            GROUP BY author, platform
            ORDER BY post_count DESC
            LIMIT ?
        """

        cursor.execute(query, params)
        results = cursor.fetchall()
        conn.close()

        authors = []
        for row in results:
            authors.append({
                'author': row[0],
                'post_count': row[1],
                'platform': row[2],
                'last_post': row[3]
            })

        return jsonify({
            'success': True,
            'data': authors,
            'count': len(authors)
        })

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/cleanup/duplicates', methods=['POST'])
def cleanup_duplicates():
    """Ê∏ÖÁêÜÈáçÂ§çÊï∞ÊçÆ"""
    try:
        # Ë∞ÉÁî®Êï∞ÊçÆÂ∫ìÂéªÈáçÂäüËÉΩ
        removed_count = db.remove_duplicates()

        return jsonify({
            'success': True,
            'data': {
                'removed_count': removed_count
            }
        })

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


if __name__ == '__main__':
    print("=" * 60)
    print("DiscussionFetcher Web Interface")
    print("=" * 60)
    print()
    print("Starting server at http://127.0.0.1:5000")
    print()
    print("Press Ctrl+C to stop")
    print("=" * 60)

    app.run(host='0.0.0.0', port=5000, debug=True)

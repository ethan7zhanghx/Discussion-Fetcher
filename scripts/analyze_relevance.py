#!/usr/bin/env python3
"""
ERNIEç›¸å…³æ€§åˆ†æå·¥å…·
ä½¿ç”¨æ–‡å¿ƒä¸€è¨€APIåˆ†æè®¨è®ºå†…å®¹æ˜¯å¦ä¸ERNIEæ–‡å¿ƒå¤§æ¨¡å‹ç›¸å…³
"""

import os
import csv
import json
import time
import requests
from datetime import datetime, date
from typing import Dict, List, Optional, Tuple
from pathlib import Path


class QianfanAPI:
    """ç™¾åº¦åƒå¸†APIå®¢æˆ·ç«¯"""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://qianfan.baidubce.com/v2/chat/completions"
        self.model = "ernie-4.5-8k-preview"  # ä½¿ç”¨ERNIE 4.5é¢„è§ˆç‰ˆ

    def _call_api_with_retry(
        self,
        messages: List[Dict[str, str]],
        max_retries: int = 3,
        initial_delay: float = 1.0
    ) -> Dict:
        """
        è°ƒç”¨APIå¹¶å®ç°é‡è¯•æœºåˆ¶

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            initial_delay: åˆå§‹å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            APIå“åº”çš„JSONæ•°æ®
        """
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "X-Appbuilder-Authorization": self.api_key
        }

        payload = {
            "model": self.model,
            "messages": messages,
            "response_format": {"type": "json_object"},
            "temperature": 0.2  # ä½æ¸©åº¦ä¿è¯åˆ¤æ–­ä¸€è‡´æ€§
        }

        delay = initial_delay
        last_error = None

        for attempt in range(max_retries + 1):
            try:
                response = requests.post(
                    self.base_url,
                    headers=headers,
                    json=payload,
                    timeout=30
                )

                # æ£€æŸ¥HTTPçŠ¶æ€
                if response.status_code == 200:
                    data = response.json()

                    # æ£€æŸ¥åƒå¸†APIé”™è¯¯
                    if data.get("error_code") or data.get("error_msg"):
                        error_msg = data.get("error_msg") or f"é”™è¯¯ç : {data.get('error_code')}"
                        raise Exception(f"åƒå¸†APIé”™è¯¯: {error_msg}")

                    # æ£€æŸ¥è¿”å›å†…å®¹
                    if not data.get("choices") or not data["choices"][0].get("message"):
                        raise Exception("APIè¿”å›æ•°æ®æ ¼å¼å¼‚å¸¸")

                    return data

                # å¤„ç†é™æµå’ŒæœåŠ¡å™¨é”™è¯¯
                elif response.status_code in [429, 500, 502, 503, 504]:
                    last_error = f"HTTP {response.status_code}: {response.text}"
                    if attempt < max_retries:
                        print(f"  âš ï¸  APIè¯·æ±‚å¤±è´¥ ({attempt + 1}/{max_retries + 1}): {last_error}")
                        print(f"  â³ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                        time.sleep(delay)
                        delay *= 2  # æŒ‡æ•°é€€é¿
                        continue
                    raise Exception(last_error)

                else:
                    raise Exception(f"HTTP {response.status_code}: {response.text}")

            except requests.exceptions.Timeout:
                last_error = "è¯·æ±‚è¶…æ—¶"
                if attempt < max_retries:
                    print(f"  âš ï¸  {last_error} ({attempt + 1}/{max_retries + 1})")
                    print(f"  â³ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    delay *= 2
                    continue
                raise Exception(last_error)

            except requests.exceptions.RequestException as e:
                last_error = f"ç½‘ç»œé”™è¯¯: {str(e)}"
                if attempt < max_retries:
                    print(f"  âš ï¸  {last_error} ({attempt + 1}/{max_retries + 1})")
                    print(f"  â³ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    delay *= 2
                    continue
                raise Exception(last_error)

        raise Exception(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæœ€åé”™è¯¯: {last_error}")

    def analyze_relevance(self, title: str, content: str) -> Dict:
        """
        åˆ†æè®¨è®ºå†…å®¹æ˜¯å¦ä¸ERNIEç›¸å…³

        Args:
            title: è®¨è®ºæ ‡é¢˜
            content: è®¨è®ºå†…å®¹

        Returns:
            åŒ…å«is_related, confidence, reasonçš„å­—å…¸
        """
        system_prompt = """ä½ æ˜¯ä¸€ä½AIæ¨¡å‹åˆ†æä¸“å®¶ï¼Œæ“…é•¿åˆ¤æ–­å†…å®¹æ˜¯å¦ä¸ERNIEæ–‡å¿ƒå¤§æ¨¡å‹ç›¸å…³ã€‚

## åˆ¤æ–­æ ‡å‡†

### æ˜ç¡®ç›¸å…³ï¼ˆconfidence >= 0.8ï¼‰
- ç›´æ¥æåˆ°"ERNIE"ã€"æ–‡å¿ƒ"ã€"æ–‡å¿ƒä¸€è¨€"ã€"Ernie"ç­‰å…³é”®è¯
- è®¨è®ºERNIEæ¨¡å‹çš„æ€§èƒ½ã€èƒ½åŠ›ã€æŠ€æœ¯ç»†èŠ‚
- ä¸ERNIEçš„å¯¹æ¯”è¯„æµ‹ï¼ˆå¦‚ vs GPTã€Claudeç­‰ï¼‰
- ERNIEçš„åº”ç”¨æ¡ˆä¾‹ã€ä½¿ç”¨ä½“éªŒ
- ç™¾åº¦å‘å¸ƒçš„ERNIEç›¸å…³å…¬å‘Šã€æ›´æ–°

### å¯èƒ½ç›¸å…³ï¼ˆconfidence 0.4-0.7ï¼‰
- æåˆ°"ç™¾åº¦"çš„AIæ¨¡å‹ï¼ˆä½†æœªæ˜ç¡®æŒ‡å‡ºERNIEï¼‰
- è®¨è®ºä¸­å›½AIå¤§æ¨¡å‹å‘å±•ï¼ˆå¯èƒ½æ¶‰åŠERNIEï¼‰
- é€šç”¨çš„AIæ¨¡å‹å¯¹æ¯”ï¼ˆERNIEå¯èƒ½åœ¨å…¶ä¸­ï¼‰

### ä¸ç›¸å…³ï¼ˆconfidence < 0.4ï¼‰
- ä»…æåˆ°ç™¾åº¦ä½†ä¸AIæ— å…³
- è®¨è®ºå…¶ä»–å…¬å¸çš„AIæ¨¡å‹ä¸”æœªæåŠERNIE
- å®Œå…¨æ— å…³çš„è¯é¢˜
- æœ‰ERNIEå…³é”®è¯ï¼Œä½†æ˜¯å¹¶ä¸æ˜¯æŒ‡çš„æ–‡å¿ƒå¤§æ¨¡å‹ï¼Œè€Œæ˜¯å…¶ä»–åŒåå¯¹è±¡

## è¾“å‡ºè¦æ±‚

ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ï¼š

{
  "is_related": trueæˆ–false,
  "confidence": 0.0åˆ°1.0ä¹‹é—´çš„æ•°å­—,
  "reason": "ç®€çŸ­çš„åˆ¤æ–­ç†ç”±ï¼ˆä¸­æ–‡ï¼Œä¸è¶…è¿‡50å­—ï¼‰"
}

æ³¨æ„ï¼š
- is_related: å½“confidence >= 0.5æ—¶ä¸ºtrueï¼Œå¦åˆ™ä¸ºfalse
- confidence: è¡¨ç¤ºç›¸å…³æ€§çš„ç½®ä¿¡åº¦ï¼Œç²¾ç¡®åˆ°å°æ•°ç‚¹å2ä½
- reason: ç®€æ˜æ‰¼è¦è¯´æ˜åˆ¤æ–­ä¾æ®ï¼Œé¿å…å†—é•¿æè¿°"""

        user_prompt = f"""è¯·åˆ¤æ–­ä»¥ä¸‹è®¨è®ºæ˜¯å¦ä¸ERNIEæ–‡å¿ƒå¤§æ¨¡å‹ç›¸å…³ï¼š

æ ‡é¢˜ï¼š{title if title and title.strip() else "æ— æ ‡é¢˜"}

å†…å®¹ï¼š{content if content and content.strip() else "æ— å†…å®¹"}

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœã€‚"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        try:
            response = self._call_api_with_retry(messages)
            content_str = response["choices"][0]["message"]["content"]

            # å¤„ç†å¯èƒ½åŒ…å«markdownä»£ç å—çš„å“åº”
            # å¦‚æœå“åº”è¢«åŒ…è£¹åœ¨```json ```ä¸­ï¼Œæå–å‡ºJSONå†…å®¹
            if content_str.strip().startswith("```"):
                # ç§»é™¤ä»£ç å—æ ‡è®°
                lines = content_str.strip().split('\n')
                # ç§»é™¤ç¬¬ä¸€è¡Œï¼ˆ```jsonï¼‰å’Œæœ€åä¸€è¡Œï¼ˆ```ï¼‰
                json_str = '\n'.join(lines[1:-1])
            else:
                json_str = content_str

            # è§£æJSON
            result = json.loads(json_str)

            # éªŒè¯å¿…éœ€å­—æ®µ
            if not all(key in result for key in ["is_related", "confidence", "reason"]):
                raise ValueError("APIè¿”å›çš„JSONç¼ºå°‘å¿…éœ€å­—æ®µ")

            # æ ‡å‡†åŒ–æ•°æ®ç±»å‹
            return {
                "is_related": bool(result["is_related"]),
                "confidence": float(result["confidence"]),
                "reason": str(result["reason"])
            }

        except json.JSONDecodeError as e:
            print(f"  âŒ JSONè§£æå¤±è´¥: {e}")
            print(f"  åŸå§‹å“åº”: {content_str}")
            # è¿”å›é»˜è®¤å€¼
            return {
                "is_related": False,
                "confidence": 0.0,
                "reason": "APIè¿”å›æ ¼å¼é”™è¯¯ï¼Œæ— æ³•è§£æ"
            }
        except Exception as e:
            print(f"  âŒ åˆ†æå¤±è´¥: {e}")
            return {
                "is_related": False,
                "confidence": 0.0,
                "reason": f"åˆ†æå‡ºé”™: {str(e)}"
            }

    def analyze_value(self, title: str, content: str) -> Dict:
        """
        åˆ†æERNIEç›¸å…³å†…å®¹æ˜¯å¦æœ‰ä»·å€¼

        Args:
            title: è®¨è®ºæ ‡é¢˜
            content: è®¨è®ºå†…å®¹

        Returns:
            åŒ…å«has_value, value_type, value_reasonçš„å­—å…¸
        """
        system_prompt = """ä½ æ˜¯ä¸€ä½å†…å®¹ä»·å€¼åˆ†æä¸“å®¶ï¼Œæ“…é•¿åˆ¤æ–­å…³äºERNIEæ–‡å¿ƒå¤§æ¨¡å‹çš„è®¨è®ºæ˜¯å¦æœ‰å®è´¨æ€§ä»·å€¼ã€‚

## æœ‰ä»·å€¼çš„å†…å®¹

### ä¼˜ç‚¹åé¦ˆï¼ˆvalue_type: "ä¼˜ç‚¹"ï¼‰
- ç”¨æˆ·ä½“éªŒåˆ°çš„å…·ä½“ä¼˜åŠ¿ï¼ˆå¦‚é€Ÿåº¦å¿«ã€ç†è§£å‡†ç¡®ç­‰ï¼‰
- ä¸å…¶ä»–æ¨¡å‹å¯¹æ¯”åå‘ç°çš„ä¼˜åŠ¿
- å…·ä½“åº”ç”¨åœºæ™¯ä¸­çš„è‰¯å¥½è¡¨ç°

### ç¼ºç‚¹åé¦ˆï¼ˆvalue_type: "ç¼ºç‚¹"ï¼‰
- ç”¨æˆ·é‡åˆ°çš„å…·ä½“é—®é¢˜æˆ–ä¸è¶³
- æ€§èƒ½ã€å‡†ç¡®åº¦ç­‰æ–¹é¢çš„ä¸è¶³
- ä¸å…¶ä»–æ¨¡å‹å¯¹æ¯”åçš„åŠ£åŠ¿

### é—®é¢˜æŠ¥å‘Šï¼ˆvalue_type: "é—®é¢˜"ï¼‰
- ä½¿ç”¨è¿‡ç¨‹ä¸­é‡åˆ°çš„å…·ä½“æŠ€æœ¯é—®é¢˜
- BugæŠ¥å‘Šæˆ–å¼‚å¸¸è¡Œä¸º
- åŠŸèƒ½é™åˆ¶æˆ–ç¼ºå¤±

### é›†æˆéœ€æ±‚ï¼ˆvalue_type: "éœ€æ±‚"ï¼‰
- å¸Œæœ›ä¸å…¶ä»–å·¥å…·/å¹³å°é›†æˆçš„éœ€æ±‚
- åŠŸèƒ½æ”¹è¿›å»ºè®®
- æ–°åŠŸèƒ½è¯·æ±‚

### æ·±åº¦è®¨è®ºï¼ˆvalue_type: "æ·±åº¦è®¨è®º"ï¼‰
- æŠ€æœ¯ç»†èŠ‚åˆ†æ
- æ¶æ„æˆ–åŸç†æ¢è®¨
- åº”ç”¨æ¡ˆä¾‹åˆ†äº«

## æ— ä»·å€¼çš„å†…å®¹

- **å®˜æ–¹é€šç¨¿**ï¼šä»…è½¬å‘å®˜æ–¹æ–°é—»ï¼Œæ— ä¸ªäººè§‚ç‚¹æˆ–ä½“éªŒ
- **æƒ…ç»ªå®£æ³„**ï¼šä»…æœ‰æƒ…ç»ªè¡¨è¾¾ï¼Œæ— å…·ä½“å†…å®¹ï¼ˆå¦‚"å¤ªæ£’äº†ï¼"ã€"å¾ˆå¤±æœ›"ï¼‰
- **ç®€å•è½¬è¿°**ï¼šä»…è½¬è¿°ä»–äººè§‚ç‚¹ï¼Œæ— æ–°ä¿¡æ¯
- **æ— å…³è®¨è®º**ï¼šè™½æåŠERNIEä½†å®é™…è®¨è®ºå…¶ä»–è¯é¢˜
- **spam/å¹¿å‘Š**ï¼šè¥é”€æ€§è´¨å†…å®¹

## è¾“å‡ºè¦æ±‚

ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ï¼š

{
  "has_value": trueæˆ–false,
  "value_type": "ä¼˜ç‚¹/ç¼ºç‚¹/é—®é¢˜/éœ€æ±‚/æ·±åº¦è®¨è®º/æ— ä»·å€¼",
  "value_reason": "ç®€çŸ­è¯´æ˜ï¼ˆä¸­æ–‡ï¼Œä¸è¶…è¿‡50å­—ï¼‰"
}

æ³¨æ„ï¼š
- has_value: trueè¡¨ç¤ºæœ‰ä»·å€¼ï¼Œfalseè¡¨ç¤ºæ— ä»·å€¼
- value_type: å¦‚æœæ— ä»·å€¼åˆ™å¡«"æ— ä»·å€¼"
- value_reason: ç®€è¦è¯´æ˜åˆ¤æ–­ä¾æ®"""

        user_prompt = f"""è¯·åˆ¤æ–­ä»¥ä¸‹å…³äºERNIEçš„è®¨è®ºæ˜¯å¦æœ‰ä»·å€¼ï¼š

æ ‡é¢˜ï¼š{title if title and title.strip() else "æ— æ ‡é¢˜"}

å†…å®¹ï¼š{content if content and content.strip() else "æ— å†…å®¹"}

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœã€‚"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        try:
            response = self._call_api_with_retry(messages)
            content_str = response["choices"][0]["message"]["content"]

            # å¤„ç†å¯èƒ½åŒ…å«markdownä»£ç å—çš„å“åº”
            if content_str.strip().startswith("```"):
                lines = content_str.strip().split('\n')
                json_str = '\n'.join(lines[1:-1])
            else:
                json_str = content_str

            # è§£æJSON
            result = json.loads(json_str)

            # éªŒè¯å¿…éœ€å­—æ®µ
            if not all(key in result for key in ["has_value", "value_type", "value_reason"]):
                raise ValueError("APIè¿”å›çš„JSONç¼ºå°‘å¿…éœ€å­—æ®µ")

            # æ ‡å‡†åŒ–æ•°æ®ç±»å‹
            return {
                "has_value": bool(result["has_value"]),
                "value_type": str(result["value_type"]),
                "value_reason": str(result["value_reason"])
            }

        except json.JSONDecodeError as e:
            print(f"  âŒ JSONè§£æå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤å€¼
            return {
                "has_value": False,
                "value_type": "æ— ä»·å€¼",
                "value_reason": "APIè¿”å›æ ¼å¼é”™è¯¯ï¼Œæ— æ³•è§£æ"
            }
        except Exception as e:
            print(f"  âŒ ä»·å€¼åˆ†æå¤±è´¥: {e}")
            return {
                "has_value": False,
                "value_type": "æ— ä»·å€¼",
                "value_reason": f"åˆ†æå‡ºé”™: {str(e)}"
            }


class RelevanceAnalyzer:
    """ERNIEç›¸å…³æ€§åˆ†æå™¨"""

    def __init__(self, api_key: str, input_csv: str, output_csv: Optional[str] = None):
        self.api = QianfanAPI(api_key)
        self.input_csv = input_csv

        # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        if output_csv is None:
            input_path = Path(input_csv)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_csv = input_path.parent / f"{input_path.stem}_analyzed_{timestamp}.csv"

        self.output_csv = output_csv
        self.progress_file = Path(output_csv).with_suffix('.progress.json')

    def load_progress(self) -> Dict:
        """åŠ è½½å¤„ç†è¿›åº¦"""
        if self.progress_file.exists():
            with open(self.progress_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {"processed_ids": [], "last_index": -1}

    def save_progress(self, progress: Dict):
        """ä¿å­˜å¤„ç†è¿›åº¦"""
        with open(self.progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)

    def analyze_csv(self, batch_size: int = 10, start_date: Optional[date] = None):
        """
        åˆ†æCSVæ–‡ä»¶ä¸­çš„æ‰€æœ‰è®¨è®º

        Args:
            batch_size: æ¯å¤„ç†å¤šå°‘æ¡æ•°æ®ä¿å­˜ä¸€æ¬¡è¿›åº¦
            start_date: åªåˆ†ææ­¤æ—¥æœŸä¹‹åçš„æ•°æ®ï¼ˆåŒ…å«å½“å¤©ï¼‰
        """
        print(f"\n{'='*60}")
        print(f"ERNIEç›¸å…³æ€§åˆ†æå·¥å…·")
        print(f"{'='*60}")
        print(f"è¾“å…¥æ–‡ä»¶: {self.input_csv}")
        print(f"è¾“å‡ºæ–‡ä»¶: {self.output_csv}")
        print(f"è¿›åº¦æ–‡ä»¶: {self.progress_file}")
        if start_date:
            print(f"æ—¥æœŸè¿‡æ»¤: åªåˆ†æ {start_date} åŠä¹‹åçš„æ•°æ®")
        print(f"{'='*60}\n")

        # è¯»å–CSV
        print("ğŸ“– æ­£åœ¨è¯»å–CSVæ–‡ä»¶...")
        with open(self.input_csv, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            rows = list(reader)
            fieldnames = reader.fieldnames

        total_rows = len(rows)
        print(f"âœ“ å…±è¯»å– {total_rows} æ¡è®¨è®º\n")

        # åŠ è½½è¿›åº¦
        progress = self.load_progress()
        processed_ids = set(progress.get("processed_ids", []))
        start_index = progress.get("last_index", -1) + 1

        if start_index > 0:
            print(f"ğŸ“Œ æ£€æµ‹åˆ°ä¸Šæ¬¡è¿›åº¦ï¼Œä»ç¬¬ {start_index + 1} æ¡ç»§ç»­å¤„ç†\n")

        # æ·»åŠ æ–°å­—æ®µ
        new_fieldnames = list(fieldnames) + ['is_ernie_related', 'relevance_score', 'relevance_reason']

        # å‡†å¤‡è¾“å‡ºæ–‡ä»¶
        output_file = open(self.output_csv, 'w', encoding='utf-8-sig', newline='')
        writer = csv.DictWriter(output_file, fieldnames=new_fieldnames)
        writer.writeheader()

        # å¦‚æœæœ‰å·²å¤„ç†çš„æ•°æ®ï¼Œå…ˆå†™å…¥
        if processed_ids:
            print(f"ğŸ“ å†™å…¥å·²å¤„ç†çš„ {len(processed_ids)} æ¡è®°å½•...")
            for i, row in enumerate(rows):
                row_id = row.get('db_id', str(i))
                if row_id in processed_ids:
                    writer.writerow(row)

        # å¤„ç†å‰©ä½™æ•°æ®
        success_count = 0
        error_count = 0
        related_count = 0
        skipped_count = 0

        start_time = time.time()

        print(f"\nğŸš€ å¼€å§‹åˆ†æ...\n")

        for i in range(start_index, total_rows):
            row = rows[i]
            row_id = row.get('db_id', str(i))

            # è·³è¿‡å·²å¤„ç†çš„
            if row_id in processed_ids:
                continue

            # æ—¥æœŸè¿‡æ»¤
            if start_date:
                created_at_str = row.get('created_at', '')
                if created_at_str:
                    try:
                        # è§£ææ—¥æœŸ (æ ¼å¼: 2025-11-14 17:34:45)
                        created_date = datetime.strptime(created_at_str.split()[0], '%Y-%m-%d').date()
                        if created_date < start_date:
                            # è·³è¿‡æ­¤æ¡æ•°æ®ï¼Œä½†ä»è¦å†™å…¥è¾“å‡º
                            row['is_ernie_related'] = 'SKIPPED'
                            row['relevance_score'] = ''
                            row['relevance_reason'] = f'æ—¥æœŸæ—©äº{start_date}'
                            writer.writerow(row)
                            skipped_count += 1
                            continue
                    except (ValueError, IndexError):
                        # æ—¥æœŸè§£æå¤±è´¥ï¼Œä»ç„¶å¤„ç†
                        pass

            title = row.get('title', '')
            content = row.get('content', '')

            print(f"[{i + 1}/{total_rows}] åˆ†æä¸­...", end=' ')

            try:
                # è°ƒç”¨APIåˆ†æ
                result = self.api.analyze_relevance(title, content)

                # æ·»åŠ åˆ†æç»“æœ
                row['is_ernie_related'] = 'YES' if result['is_related'] else 'NO'
                row['relevance_score'] = f"{result['confidence']:.2f}"
                row['relevance_reason'] = result['reason']

                # å†™å…¥ç»“æœ
                writer.writerow(row)
                output_file.flush()  # ç«‹å³å†™å…¥ç£ç›˜

                # æ›´æ–°ç»Ÿè®¡
                success_count += 1
                if result['is_related']:
                    related_count += 1
                    print(f"âœ“ ç›¸å…³ (ç½®ä¿¡åº¦: {result['confidence']:.2f})")
                else:
                    print(f"â—‹ ä¸ç›¸å…³ (ç½®ä¿¡åº¦: {result['confidence']:.2f})")

                # æ›´æ–°è¿›åº¦
                processed_ids.add(row_id)

                # å®šæœŸä¿å­˜è¿›åº¦
                if (i + 1) % batch_size == 0:
                    progress = {
                        "processed_ids": list(processed_ids),
                        "last_index": i
                    }
                    self.save_progress(progress)

                    elapsed = time.time() - start_time
                    avg_time = elapsed / success_count if success_count > 0 else 0
                    remaining = (total_rows - i - 1) * avg_time

                    print(f"\n  ğŸ’¾ è¿›åº¦å·²ä¿å­˜ | å·²å¤„ç†: {i + 1}/{total_rows} | "
                          f"ç›¸å…³: {related_count} | é¢„è®¡å‰©ä½™: {remaining/60:.1f}åˆ†é’Ÿ\n")

                # APIé™æµæ§åˆ¶ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(0.5)

            except Exception as e:
                error_count += 1
                print(f"âŒ é”™è¯¯: {e}")

                # å†™å…¥é”™è¯¯æ ‡è®°
                row['is_ernie_related'] = 'ERROR'
                row['relevance_score'] = '0.00'
                row['relevance_reason'] = f"å¤„ç†å¤±è´¥: {str(e)}"
                writer.writerow(row)
                output_file.flush()

                processed_ids.add(row_id)

        output_file.close()

        # å®Œæˆï¼Œåˆ é™¤è¿›åº¦æ–‡ä»¶
        if self.progress_file.exists():
            self.progress_file.unlink()

        # è¾“å‡ºç»Ÿè®¡
        elapsed = time.time() - start_time
        print(f"\n{'='*60}")
        print(f"âœ… åˆ†æå®Œæˆï¼")
        print(f"{'='*60}")
        print(f"æ€»è®¡: {total_rows} æ¡")
        if skipped_count > 0:
            print(f"å·²è·³è¿‡ï¼ˆæ—¥æœŸè¿‡æ»¤ï¼‰: {skipped_count} æ¡")
        print(f"å·²åˆ†æ: {success_count} æ¡")
        print(f"å¤±è´¥: {error_count} æ¡")
        if success_count > 0:
            print(f"ERNIEç›¸å…³: {related_count} æ¡ ({related_count/success_count*100:.1f}%)")
        print(f"è€—æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")
        print(f"\nç»“æœå·²ä¿å­˜è‡³: {self.output_csv}")
        print(f"{'='*60}\n")


def main():
    """ä¸»å‡½æ•°"""
    # API Key
    api_key = "bce-v3/ALTAK-3t7fjMhp5Bx2KUqiEj4SF/8b44c3fc85f248a4b9b1c7532d0d2fc3f91150bc"

    # è¾“å…¥æ–‡ä»¶
    input_csv = "data/exports/discussions_ERNIE_20251117_184548.csv"

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_csv):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ {input_csv}")
        return

    # åˆ›å»ºåˆ†æå™¨
    analyzer = RelevanceAnalyzer(
        api_key=api_key,
        input_csv=input_csv
    )

    # è®¾ç½®èµ·å§‹æ—¥æœŸï¼š11æœˆ1æ—¥
    filter_start_date = date(2025, 11, 1)

    # æ‰§è¡Œåˆ†æ
    try:
        analyzer.analyze_csv(batch_size=10, start_date=filter_start_date)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œè¿›åº¦å·²ä¿å­˜ï¼Œä¸‹æ¬¡è¿è¡Œå°†ç»§ç»­å¤„ç†")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        raise


if __name__ == "__main__":
    main()

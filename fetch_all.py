#!/usr/bin/env python3
"""ä¸€æ¬¡æ€§æ‹‰å–æ‰€æœ‰å¹³å°æ•°æ®"""

import argparse
import os
from pathlib import Path
from src.reddit import RedditFetcher
from src.huggingface import HuggingFaceFetcher


def main():
    parser = argparse.ArgumentParser(description='æŠ“å– Redditã€HuggingFace çš„è®¨è®ºæ•°æ®')

    # æ–°çš„ --sources å‚æ•°ï¼ˆæ¨èä½¿ç”¨ï¼‰
    parser.add_argument(
        '--sources',
        nargs='+',
        choices=['praw', 'selenium', 'huggingface', 'all'],
        default=None,
        help=(
            'è¦æŠ“å–çš„æ•°æ®æºï¼ˆæ¨èä½¿ç”¨æ­¤å‚æ•°ï¼‰ï¼š\n'
            '  praw       - Reddit PRAW API (posts + comments)\n'
            '  selenium   - Reddit Selenium æœç´¢é¡µé¢è¯„è®ºï¼ˆéœ€è¦ cookiesï¼‰\n'
            '  huggingface - HuggingFace æ¨¡å‹è®¨è®º\n'
            '  all        - å…¨éƒ¨æ•°æ®æº\n'
            'ç¤ºä¾‹: --sources selenium (åªçˆ¬ Selenium)\n'
            '      --sources praw huggingface (çˆ¬ PRAW å’Œ HF)'
        )
    )

    # æ—§å‚æ•°ï¼ˆå‘åå…¼å®¹ï¼Œä½†ä¸æ¨èä½¿ç”¨ï¼‰
    parser.add_argument(
        '--platforms',
        nargs='+',
        choices=['reddit', 'huggingface', 'all'],
        default=None,
        help='[å·²å¼ƒç”¨] ä½¿ç”¨ --sources æ›¿ä»£'
    )
    parser.add_argument(
        '--reddit-comments',
        action='store_true',
        help='[å·²å¼ƒç”¨] ä½¿ç”¨ --sources selenium æ›¿ä»£'
    )

    parser.add_argument(
        '--query',
        type=str,
        default='ERNIE',
        help='æœç´¢å…³é”®è¯ï¼ˆé»˜è®¤: ERNIEï¼‰ã€‚ç¤ºä¾‹: --query "PaddleOCR-VL"'
    )
    parser.add_argument(
        '--reddit-mode',
        choices=['subreddits', 'global'],
        default='subreddits',
        help=(
            'Reddit æœç´¢æ–¹å¼ï¼ˆé»˜è®¤: subredditsï¼‰ï¼š\n'
            '  subreddits - åœ¨ç‰¹å®šå­ç‰ˆå—ä¸­æœç´¢ï¼ˆ9ä¸ªAIç›¸å…³ç‰ˆå—ï¼‰\n'
            '  global     - å…¨å±€æœç´¢æ•´ä¸ªRedditï¼ˆé€‚åˆå°ä¼—è¯é¢˜å¦‚ PaddleOCR-VLï¼‰\n'
            'ç¤ºä¾‹: --reddit-mode global'
        )
    )
    parser.add_argument(
        '--cookies',
        type=str,
        default='./cookies.json',
        help='Cookies æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äº Selenium æŠ“å–ï¼‰ã€‚é»˜è®¤: ./cookies.json'
    )
    parser.add_argument(
        '--max-pages',
        type=int,
        default=50,
        help='Selenium æ¯ä¸ªæ¿å—æœ€å¤šæ»šåŠ¨æ¬¡æ•°ï¼ˆé»˜è®¤: 50ï¼Œçº¦1250æ¡è¯„è®º/æ¿å—ï¼‰'
    )
    parser.add_argument(
        '--replace-more-limit',
        type=int,
        default=0,
        help='PRAW è¯„è®ºå±•å¼€é™åˆ¶ï¼ˆé»˜è®¤: 0 = å…¨éƒ¨å±•å¼€ï¼‰ã€‚è®¾ä¸º None åˆ™ä¸å±•å¼€'
    )
    parser.add_argument(
        '--export',
        action='store_true',
        help='æŠ“å–å®Œæˆåè‡ªåŠ¨å¯¼å‡ºä¸º CSV æ–‡ä»¶'
    )
    parser.add_argument(
        '--export-format',
        choices=['csv', 'excel'],
        default='csv',
        help='å¯¼å‡ºæ ¼å¼ï¼ˆé»˜è®¤: csvï¼‰'
    )
    parser.add_argument(
        '--days',
        type=int,
        default=None,
        help='åªè·å–æœ€è¿‘ N å¤©çš„æ•°æ®ï¼ˆé»˜è®¤: None = å…¨éƒ¨å†å²æ•°æ®ï¼‰'
    )

    args = parser.parse_args()

    # ç¡®å®šè¦æŠ“å–çš„æ•°æ®æº
    sources = []

    if args.sources:
        # ä½¿ç”¨æ–°çš„ --sources å‚æ•°
        if 'all' in args.sources:
            sources = ['praw', 'selenium', 'huggingface']
        else:
            sources = args.sources
    else:
        # å‘åå…¼å®¹æ—§å‚æ•°
        if args.platforms:
            if 'all' in args.platforms:
                platforms = ['reddit', 'huggingface']
            else:
                platforms = args.platforms

            if 'reddit' in platforms:
                sources.append('praw')
                if args.reddit_comments:
                    sources.append('selenium')
            if 'huggingface' in platforms:
                sources.append('huggingface')
        else:
            # é»˜è®¤ï¼šå…¨éƒ¨
            sources = ['praw', 'selenium', 'huggingface']

    # å»é‡
    sources = list(dict.fromkeys(sources))

    print("=" * 60)
    print(f"å¼€å§‹æŠ“å–æ•°æ® - æ•°æ®æº: {', '.join(sources)}")
    print("=" * 60)

    results = {}

    # Reddit - PRAW API (posts + comments)
    if 'praw' in sources:
        print("\n[PRAW] æŠ“å– Reddit å¸–å­å’Œè¯„è®º (PRAW API)...")
        print(f"æœç´¢å…³é”®è¯: {args.query}")
        print(f"æœç´¢æ–¹å¼: {'å…¨å±€æœç´¢ (æ•´ä¸ªReddit)' if args.reddit_mode == 'global' else 'å­ç‰ˆå—æœç´¢ (9ä¸ªAIç‰ˆå—)'}")

        if args.reddit_mode == 'subreddits':
            print("æ¿å— (9ä¸ª):")
            print("  - LocalLLM, LocalLlaMa, ChatGPT")
            print("  - ArtificialIntelligence, OpenSourceeAI")
            print("  - singularity, machinelearningnews")
            print("  - SillyTavernAI, StableDiffusion")

        if args.days:
            print(f"  æ—¶é—´èŒƒå›´: æœ€è¿‘ {args.days} å¤©")
        print(f"  è¯„è®ºå±•å¼€é™åˆ¶: {args.replace_more_limit} (0=å…¨éƒ¨å±•å¼€)")

        reddit = RedditFetcher(verbose=True)

        if args.reddit_mode == 'global':
            # å…¨å±€æœç´¢æ¨¡å¼
            print("\nå¼€å§‹å…¨å±€æœç´¢...")
            posts = reddit.search_all_reddit(
                query=args.query,
                limit=100,
                search_keywords=args.query
            )

            # æ—¶é—´è¿‡æ»¤ - Posts
            if args.days:
                from datetime import datetime, timedelta
                cutoff_date = datetime.now() - timedelta(days=args.days)
                posts = [p for p in posts if p.created_at >= cutoff_date]

            discussions = posts.copy()

            # è·å–è¯„è®º
            print(f"\nè·å– {len(posts)} ä¸ªå¸–å­çš„è¯„è®º...")
            all_comments = []
            for idx, post in enumerate(posts, 1):
                print(f"  [{idx}/{len(posts)}] {post.title[:50]}...")
                comments = reddit.fetch_post_comments(
                    post_id=post.id,
                    post_title=post.title,
                    subreddit_name=post.subreddit,
                    search_keywords=args.query,
                    replace_more_limit=args.replace_more_limit
                )

                # æ—¶é—´è¿‡æ»¤ - Comments
                if args.days:
                    comments = [c for c in comments if c.created_at >= cutoff_date]

                all_comments.extend(comments)

            if all_comments:
                reddit.add_discussions(all_comments, source='api')
                discussions.extend(all_comments)
        else:
            # å­ç‰ˆå—æœç´¢æ¨¡å¼ï¼ˆåŸæœ‰æ–¹å¼ï¼‰
            discussions = reddit.fetch(
                query=args.query,
                fetch_comments=True,
                days_limit=args.days,
                replace_more_limit=args.replace_more_limit
            )

        # ç»Ÿè®¡ posts å’Œ comments
        from src.models import ContentType
        posts_count = sum(1 for d in discussions if d.content_type == ContentType.POST)
        comments_count = sum(1 for d in discussions if d.content_type == ContentType.COMMENT)

        results['Reddit Posts (PRAW)'] = posts_count
        results['Reddit Comments (PRAW)'] = comments_count
        print(f"âœ“ Reddit Posts: {posts_count} æ¡")
        print(f"âœ“ Reddit Comments (from posts): {comments_count} æ¡")

    # Reddit - Selenium æœç´¢é¡µé¢è¯„è®º
    if 'selenium' in sources:
        print(f"\n[Selenium] æŠ“å– Reddit æœç´¢é¡µé¢è¯„è®ºï¼ˆSelenium + Cookiesï¼‰...")
        print(f"æœç´¢å…³é”®è¯: {args.query}")
        print(f"æœç´¢æ–¹å¼: {'å…¨å±€æœç´¢ (æ•´ä¸ªReddit)' if args.reddit_mode == 'global' else 'å­ç‰ˆå—æœç´¢ (9ä¸ªAIç‰ˆå—)'}")
        print("è¯´æ˜ï¼šä»æœç´¢é¡µé¢æ»šåŠ¨åŠ è½½è¯„è®ºï¼ˆæŒ‰æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°è¯„è®ºï¼‰")

        if args.reddit_mode == 'subreddits':
            print("æ¿å— (9ä¸ª): åŒä¸Š")

        if args.days:
            print(f"  æ—¶é—´èŒƒå›´: æœ€è¿‘ {args.days} å¤©")
        else:
            print(f"  æ—¶é—´èŒƒå›´: æœ€è¿‘ 30 å¤©ï¼ˆé»˜è®¤ï¼‰")

        if args.reddit_mode == 'global':
            print(f"  æœ€å¤§æ»šåŠ¨æ¬¡æ•°: {args.max_pages} æ¬¡")
        else:
            print(f"  æœ€å¤§æ»šåŠ¨æ¬¡æ•°: {args.max_pages} æ¬¡/æ¿å—")

        if not Path(args.cookies).exists():
            print(f"âŒ æœªæ‰¾åˆ° cookies æ–‡ä»¶: {args.cookies}")
            print("\nå¦‚ä½•å¯¼å‡º cookies:")
            print("  python3 -m src.reddit_comments_selenium guide")
            print("\nè·³è¿‡ Selenium æŠ“å–...")
        else:
            try:
                from src.reddit_comments_selenium import RedditCommentsSeleniumFetcher

                comments_fetcher = RedditCommentsSeleniumFetcher(
                    cookies_file=args.cookies,
                    headless=True,  # æ— å¤´æ¨¡å¼
                    verbose=True
                )
                # ä½¿ç”¨ --days å‚æ•°ï¼Œå¦‚æœæœªæŒ‡å®šåˆ™é»˜è®¤ 30 å¤©
                days_limit = args.days if args.days else 30
                comments_fetcher.fetch(
                    query=args.query,
                    max_scrolls=args.max_pages,
                    days_limit=days_limit,
                    search_mode=args.reddit_mode,
                    search_keywords=args.query
                )
                results['Reddit Comments (Selenium)'] = len(comments_fetcher.discussions)
                mode_str = "å…¨å±€æœç´¢" if args.reddit_mode == 'global' else f"æ»šåŠ¨ {args.max_pages} æ¬¡/æ¿å—"
                print(f"âœ“ Reddit Search Comments: {results['Reddit Comments (Selenium)']} æ¡ï¼ˆ{mode_str}ï¼Œæœ€è¿‘{days_limit}å¤©ï¼‰")
                print(f"  æç¤ºï¼šæ•°æ®åº“å·²è‡ªåŠ¨å»é‡ï¼Œé‡å¤çš„è¯„è®ºåªä¿ç•™ä¸€ä»½")

            except Exception as e:
                print(f"âŒ Selenium æŠ“å–å¤±è´¥: {e}")
                print("æç¤º: cookies å¯èƒ½å·²è¿‡æœŸï¼Œæˆ– Chrome æœªå®‰è£…")
                import traceback
                traceback.print_exc()

    # HuggingFace
    if 'huggingface' in sources:
        print("\n[HuggingFace] æŠ“å– HuggingFace æ¨¡å‹è®¨è®º...")
        print(f"æœç´¢å…³é”®è¯: {args.query}")
        if args.days:
            print(f"  æ—¶é—´èŒƒå›´: æœ€è¿‘ {args.days} å¤©")
        hf = HuggingFaceFetcher(verbose=True)
        hf.fetch(args.query, days_limit=args.days)
        results['HuggingFace'] = len(hf.discussions)
        print(f"âœ“ HuggingFace: {results['HuggingFace']} æ¡")

    # æ€»è®¡
    if results:
        total = sum(results.values())
        print("\n" + "=" * 60)
        print("âœ“ å®Œæˆï¼")
        for source, count in results.items():
            print(f"  {source}: {count} æ¡")
        print(f"  æ€»è®¡: {total} æ¡æ•°æ®å·²ä¿å­˜åˆ°æ•°æ®åº“")
        print("=" * 60)
    else:
        print("\n" + "=" * 60)
        print("âš ï¸  æœªé€‰æ‹©ä»»ä½•æ•°æ®æºæˆ–æŠ“å–å¤±è´¥")
        print("=" * 60)

    # æç¤º
    print("\nğŸ’¡ ä½¿ç”¨æç¤º:")
    print("  åŸºç¡€ç”¨æ³•:")
    print("    åªçˆ¬ Selenium:      python3 fetch_all.py --sources selenium")
    print("    åªçˆ¬ PRAW:          python3 fetch_all.py --sources praw")
    print("    åªçˆ¬ HuggingFace:   python3 fetch_all.py --sources huggingface")
    print("    çˆ¬ PRAW + Selenium: python3 fetch_all.py --sources praw selenium")
    print("    çˆ¬å…¨éƒ¨:             python3 fetch_all.py --sources all")
    print()
    print("  è‡ªå®šä¹‰æœç´¢å…³é”®è¯:")
    print("    python3 fetch_all.py --query \"PaddleOCR-VL\" --sources praw")
    print("    python3 fetch_all.py --query \"ERNIE-4.5\" --sources all")
    print()
    print("  Reddit æœç´¢æ–¹å¼:")
    print("    å­ç‰ˆå—æœç´¢ (é»˜è®¤):  python3 fetch_all.py --query \"ERNIE\" --reddit-mode subreddits")
    print("    å…¨å±€æœç´¢:           python3 fetch_all.py --query \"PaddleOCR-VL\" --reddit-mode global")
    print()
    print("  é«˜çº§é€‰é¡¹:")
    print("    ç»“åˆæ—¶é—´è¿‡æ»¤:       python3 fetch_all.py --sources selenium --days 7")
    print("    è‡ªå®šä¹‰æ»šåŠ¨æ¬¡æ•°:     python3 fetch_all.py --sources selenium --max-pages 100")
    print("    è¯„è®ºå±•å¼€é™åˆ¶:       python3 fetch_all.py --sources praw --replace-more-limit 0")

    # è‡ªåŠ¨å¯¼å‡º
    if args.export and results:
        from src.database import DatabaseManager
        from datetime import datetime

        print("\n" + "=" * 60)
        print("å¯¼å‡ºæ•°æ®")
        print("=" * 60)

        db = DatabaseManager()
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        if args.export_format == 'csv':
            output_file = f'./data/exports/discussions_{timestamp}.csv'
            db.export_to_csv(output_file)
        else:
            output_file = f'./data/exports/discussions_{timestamp}.xlsx'
            db.export_to_excel(output_file)

        print(f"âœ“ æ•°æ®å·²å¯¼å‡ºåˆ°: {output_file}")


if __name__ == '__main__':
    main()

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from typing import List, Dict, Optional\nfrom datetime import datetime\nimport praw\nfrom prawcore.exceptions import PrawcoreException, ResponseException\nimport logging\nimport time\n\nlogging.basicConfig(level=logging.WARNING)\nlogger = logging.getLogger(__name__)\n\n\nclass RedditERNIEFetcher:\n    \"\"\"用于获取 Reddit 板块的所有讨论和评论\"\"\"\n    \n    def __init__(\n        self,\n        client_id: str,\n        client_secret: str,\n        user_agent: str = \"ERNIE_Discussion_Fetcher/1.0\",\n        verbose: bool = False\n    ):\n        try:\n            self.reddit = praw.Reddit(\n                client_id=client_id,\n                client_secret=client_secret,\n                user_agent=user_agent,\n                check_for_async=False\n            )\n            self.verbose = verbose\n        except Exception as e:\n            logger.error(f\"Reddit 连接失败: {e}\")\n            raise\n    \n    def _convert_timestamp(self, timestamp: float) -> str:\n        \"\"\"将 Unix 时间戳转换为可读的日期格式\"\"\"\n        return datetime.fromtimestamp(timestamp).strftime(\"%Y-%m-%d %H:%M:%S\")\n    \n    def fetch_subreddit_posts(\n        self,\n        subreddit_name: str,\n        time_filter: Optional[str] = None,\n        sort_by: str = \"new\",\n        limit: Optional[int] = None\n    ) -> List[Dict]:\n        \"\"\"\n        获取某个板块的所有帖子\n        \n        Args:\n            subreddit_name: 板块名称（如 \"LanguageModels\"）\n            time_filter: 时间过滤 (\"day\", \"week\", \"month\", \"year\", \"all\")\n            sort_by: 排序方式 (\"new\", \"hot\", \"top\", \"rising\")\n            limit: 获取数量限制（None = 无限制，实际受 Reddit API 限制）\n            \n        Returns:\n            帖子数据列表\n        \"\"\"\n        posts = []\n        \n        try:\n            subreddit = self.reddit.subreddit(subreddit_name)\n            \n            # 根据排序方式获取帖子\n            if sort_by == \"new\":\n                submissions = subreddit.new(limit=limit)\n            elif sort_by == \"hot\":\n                submissions = subreddit.hot(limit=limit)\n            elif sort_by == \"top\":\n                submissions = subreddit.top(time_filter=time_filter or \"all\", limit=limit)\n            elif sort_by == \"rising\":\n                submissions = subreddit.rising(limit=limit)\n            else:\n                submissions = subreddit.new(limit=limit)\n            \n            for submission in submissions:\n                posts.append({\n                    \"type\": \"post\",\n                    \"post_id\": submission.id,\n                    \"title\": submission.title,\n                    \"subreddit\": submission.subreddit.display_name,\n                    \"author\": submission.author.name if submission.author else \"[deleted]\",\n                    \"score\": submission.score,\n                    \"upvote_ratio\": submission.upvote_ratio,\n                    \"num_comments\": submission.num_comments,\n                    \"created_at\": self._convert_timestamp(submission.created_utc),\n                    \"url\": submission.url,\n                    \"permalink\": f\"https://reddit.com{submission.permalink}\",\n                    \"content\": submission.selftext,\n                    \"is_self\": submission.is_self,\n                    \"link_flair_text\": submission.link_flair_text,\n                    \"parent_post_id\": None,\n                    \"parent_title\": None\n                })\n                \n                time.sleep(0.1)  # 避免速率限制\n            \n            if self.verbose:\n                print(f\"✓ r/{subreddit_name}: 获取到 {len(posts)} 个帖子\")\n                \n        except ResponseException as e:\n            logger.error(f\"获取 r/{subreddit_name} 失败 (401): {e}\")\n        except PrawcoreException as e:\n            logger.warning(f\"获取 r/{subreddit_name} 失败: {e}\")\n        except Exception as e:\n            logger.error(f\"处理 r/{subreddit_name} 时出错: {e}\")\n        \n        return posts\n    \n    def fetch_comments_for_post(self, post_url: str) -> List[Dict]:\n        \"\"\"\n        获取某个帖子的所有评论\n        \n        Args:\n            post_url: 帖子的 URL 或 permalink\n            \n        Returns:\n            评论数据列表\n        \"\"\"\n        comments = []\n        \n        try:\n            submission = self.reddit.submission(url=post_url)\n            \n            # 展开所有评论（limit=None 表示全部展开）\n            submission.comments.replace_more(limit=None)\n            \n            for comment in submission.comments.list():\n                if comment.author:  # 排除已删除的评论\n                    comments.append({\n                        \"type\": \"comment\",\n                        \"post_id\": submission.id,\n                        \"comment_id\": comment.id,\n                        \"title\": None,\n                        \"subreddit\": submission.subreddit.display_name,\n                        \"author\": comment.author.name,\n                        \"score\": comment.score,\n                        \"upvote_ratio\": None,\n                        \"num_comments\": None,\n                        \"created_at\": self._convert_timestamp(comment.created_utc),\n                        \"url\": f\"https://reddit.com{comment.permalink}\",\n                        \"permalink\": comment.permalink,\n                        \"content\": comment.body,\n                        \"is_self\": True,\n                        \"link_flair_text\": None,\n                        \"parent_post_id\": submission.id,\n                        \"parent_title\": submission.title\n                    })\n        \n        except Exception as e:\n            logger.error(f\"获取评论失败: {e}\")\n        \n        return comments\n    \n    def fetch_subreddit_full(\n        self,\n        subreddit_name: str,\n        time_filter: Optional[str] = None,\n        sort_by: str = \"new\",\n        post_limit: Optional[int] = None,\n        fetch_comments: bool = True\n    ) -> List[Dict]:\n        \"\"\"\n        获取板块的完整数据（帖子 + 评论）\n        \n        Args:\n            subreddit_name: 板块名称\n            time_filter: 时间过滤\n            sort_by: 排序方式\n            post_limit: 帖子数量限制\n            fetch_comments: 是否获取评论\n            \n        Returns:\n            包含帖子和评论的完整数据列表\n        \"\"\"\n        all_data = []\n        \n        if self.verbose:\n            print(f\"\\n{'='*50}\")\n            print(f\"正在获取 r/{subreddit_name} 的数据...\")\n            print(f\"{'='*50}\")\n        \n        # 获取帖子\n        posts = self.fetch_subreddit_posts(\n            subreddit_name,\n            time_filter=time_filter,\n            sort_by=sort_by,\n            limit=post_limit\n        )\n        \n        all_data.extend(posts)\n        \n        # 获取评论\n        if fetch_comments:\n            for idx, post in enumerate(posts, 1):\n                if self.verbose and idx % 10 == 0:\n                    print(f\"  处理评论进度: {idx}/{len(posts)}\")\n                \n                comments = self.fetch_comments_for_post(post['permalink'])\n                all_data.extend(comments)\n                \n                time.sleep(0.5)  # 避免速率限制\n        \n        if self.verbose:\n            posts_count = len([d for d in all_data if d['type'] == 'post'])\n            comments_count = len([d for d in all_data if d['type'] == 'comment'])\n            print(f\"✓ 完成！共 {posts_count} 个帖子，{comments_count} 条评论\\n\")\n        \n        return all_data\n    \n    def fetch_multiple_subreddits(\n        self,\n        subreddit_names: List[str],\n        time_filter: Optional[str] = None,\n        sort_by: str = \"new\",\n        post_limit: Optional[int] = None,\n        fetch_comments: bool = True\n    ) -> List[Dict]:\n        \"\"\"\n        批量获取多个板块的完整数据\n        \n        Args:\n            subreddit_names: 板块名称列表\n            time_filter: 时间过滤\n            sort_by: 排序方式\n            post_limit: 每个板块的帖子数量限制\n            fetch_comments: 是否获取评论\n            \n        Returns:\n            所有板块的数据汇总\n        \"\"\"\n        all_data = []\n        \n        for subreddit_name in subreddit_names:\n            data = self.fetch_subreddit_full(\n                subreddit_name,\n                time_filter=time_filter,\n                sort_by=sort_by,\n                post_limit=post_limit,\n                fetch_comments=fetch_comments\n            )\n            all_data.extend(data)\n        \n        return all_data\n\n\nif __name__ == \"__main__\":\n    import pandas as pd\n    \n    client_id = \"1KiqUsgcGQDRiXvTgU32Ow\"\n    client_secret = \"56PQoBZJ43HEj2sBWYG9pN_UggWnYw\"\n    user_agent = \"ERNIE_Discussion_Fetcher/1.0\"\n    \n    fetcher = RedditERNIEFetcher(\n        client_id=client_id,\n        client_secret=client_secret,\n        user_agent=user_agent,\n        verbose=True\n    )\n    \n    # 定义关键板块\n    key_subreddits = [\n        \"LocalLLM\",\n        \"LocalLlaMa\",\n        \"ChatGPT\",\n        \"ArtificialIntelligence\",\n        \"OpenSourceeAI\",\n        \"singularity\",\n        \"machinelearningnews\",\n        \"SillyTavernAI\",\n        \"StableDiffusion\"\n    ]\n    \n    # 方案 1：首次全量获取（不限时间）\n    print(\"首次全量获取...\")\n    data = fetcher.fetch_multiple_subreddits(\n        key_subreddits,\n        time_filter=None,  # 全量\n        sort_by=\"new\",\n        post_limit=1000,  # 每个板块最多1000个帖子\n        fetch_comments=True\n    )\n    \n    # 方案 2：每周增量更新（只获取最近一周）\n    # print(\"每周增量更新...\")\n    # data = fetcher.fetch_multiple_subreddits(\n    #     key_subreddits,\n    #     time_filter=\"week\",\n    #     sort_by=\"new\",\n    #     post_limit=100,\n    #     fetch_comments=True\n    # )\n    \n    # 保存数据\n    df = pd.DataFrame(data)\n    \n    if not df.empty:\n        # 在本地过滤出包含 ERNIE 的数据\n        mask = df.apply(\n            lambda row: 'ernie' in str(row.get('title', '')).lower() or \n                       'ernie' in str(row.get('content', '')).lower(),\n            axis=1\n        )\n        df_filtered = df[mask]\n        \n        print(f\"\\n总数据: {len(df)} 条\")\n        print(f\"包含 ERNIE 的数据: {len(df_filtered)} 条\")\n        \n        # 保存所有数据\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        df.to_csv(f\"reddit_all_{timestamp}.csv\", index=False, encoding='utf-8-sig')\n        \n        # 保存过滤后的数据\n        if not df_filtered.empty:\n            df_filtered.to_csv(f\"reddit_ernie_{timestamp}.csv\", index=False, encoding='utf-8-sig')\n            print(f\"\\n已保存到: reddit_ernie_{timestamp}.csv\")\n    else:\n        print(\"未获取到任何数据\")"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"Reddit.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "认证成功！只读模式: True\n",
      "测试获取 r/test: test\n"
     ]
    }
   ],
   "source": [
    "# 测试连接\n",
    "import praw\n",
    "\n",
    "client_id = \"1KiqUsgcGQDRiXvTgU32Ow\"\n",
    "client_secret = \"56PQoBZJ43HEj2sBWYG9pN_UggWnYw\"\n",
    "user_agent = \"ERNIE_fetcher/1.0 by YOUR_REDDIT_USERNAME\"  # 这里需要改\n",
    "\n",
    "try:\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=client_id,\n",
    "        client_secret=client_secret,\n",
    "        user_agent=user_agent\n",
    "    )\n",
    "    \n",
    "    # 测试连接\n",
    "    print(f\"认证成功！只读模式: {reddit.read_only}\")\n",
    "    print(f\"测试获取 r/test: {reddit.subreddit('test').display_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"认证失败: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}